{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7397938d",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bd6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running example...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running example...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf28737",
   "metadata": {},
   "source": [
    "## Importing local code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2894d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this at the top of your notebook so that external\n",
    "# code changes are imported automatically.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6cb4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from my_project.example import example\n",
    "print(example())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7951f",
   "metadata": {},
   "source": [
    "## Load environment variables\n",
    "\n",
    "Environment variables are stored in your `.env` file at the repository root. You can load them using the dotenv package. This allows you to store user secrets and other information related to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2770a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dotenv file. Environment PROJECT=my_project\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "if not dotenv.load_dotenv():\n",
    "    print(\"Could not find dotenv file...\")\n",
    "else:\n",
    "    print(\"Successfully loaded dotenv file. \", end=\"\")\n",
    "    print(f\"Environment PROJECT={os.environ.get('PROJECT')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd4d1e",
   "metadata": {},
   "source": [
    "## Basic PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d2e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 4\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation\n",
      "Successfully created tensor on GPU 0\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation\n",
      "Successfully created tensor on GPU 1\n",
      "GPU 2: NVIDIA RTX 6000 Ada Generation\n",
      "Successfully created tensor on GPU 2\n",
      "GPU 3: NVIDIA RTX 6000 Ada Generation\n",
      "Successfully created tensor on GPU 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_gpus():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "            # Allocate a tensor on each GPU to test if it's functioning properly\n",
    "            try:\n",
    "                x = torch.randn((1000, 1000), device=f\"cuda:{i}\")\n",
    "                print(f\"Successfully created tensor on GPU {i}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to create tensor on GPU {i}: {e}\")\n",
    "    else:\n",
    "        print(\"No GPUs are available.\")\n",
    "\n",
    "test_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7418161",
   "metadata": {},
   "source": [
    "## Huggingface\n",
    "\n",
    "The docker file mounts the shared huggingface cache; you can see it's location inside the dockerfile the `HF_HUB_CACHE` variable. Outside of the dockerfile, it lives at `/mnt/raid/data/shared-cache/huggingface`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92d91366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging face cache inside the dockerfile lives at /cache/huggingface\n",
      "Hugging face token is hf_...fiR\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the environment variables...\n",
    "print(\n",
    "    f\"Hugging face cache inside the dockerfile \"\n",
    "    f\"lives at {os.environ.get('HF_HUB_CACHE')}\"\n",
    ")\n",
    "secret_token = os.environ.get(\"HF_TOKEN\")\n",
    "if secret_token is None:\n",
    "    raise ValueError(\n",
    "        \"Missing HF_TOKEN environment variable. \"\n",
    "        \"Set it in your .env file and run load_dotenv().\"\n",
    "    )\n",
    "\n",
    "redacted = f\"{secret_token[:3]}...{secret_token[-3:]}\"\n",
    "print(f\"Hugging face token is {redacted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54557fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0824,  0.0667, -0.2880,  ..., -0.3566,  0.1960,  0.5381],\n",
       "         [ 0.0310, -0.1448,  0.0952,  ..., -0.1560,  1.0151,  0.0947],\n",
       "         [-0.8935,  0.3240,  0.4184,  ..., -0.5498,  0.2853,  0.1149],\n",
       "         ...,\n",
       "         [-0.2812, -0.8531,  0.6912,  ..., -0.5051,  0.4716, -0.6854],\n",
       "         [-0.4429, -0.7820, -0.8055,  ...,  0.1949,  0.1081,  0.0130],\n",
       "         [ 0.5570, -0.1080, -0.2412,  ...,  0.2817, -0.3996, -0.1882]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())\n",
    "outputs['last_hidden_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24620614",
   "metadata": {},
   "source": [
    "## Fancier model: Llama2-13b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f24f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
